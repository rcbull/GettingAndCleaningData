idx <- grep("mean\\(\\)|std\\(\\)", readLines(conn))
close(conn)
# function df:
# this function is written to process the files in the train and
# test folders into proper data frames
make.df <- function(directory, index = idx){
subject.df <- read.table(sprintf("%s/%s/subject_%s.txt",
folder,directory, directory))
y.df <- read.table(sprintf("%s/%s/y_%s.txt",
folder, directory, directory))
X.df <- read.table(sprintf("%s/%s/X_%s.txt",
folder, directory, directory))[, index]
dataset <- cbind(subject.df, y.df, X.df)
# merge activity labels to sensor data
activity.df <- read.table(sprintf("%s/activity_labels.txt", folder))
suppressWarnings(dataset <- merge(dataset, activity.df, by.x = 2,
by.y = 1))
dataset[1] <- NULL  # erase column because no longer needed
# reorder columns in order of subject, activity, then feature
dataset <- dataset[c(1, dim(dataset)[2], seq(2, dim(dataset)[2]-1))]
dataset
}
# combined the train and test data frames
train.df <- make.df("train")
test.df <- make.df("test")
clean.df <- rbind(train.df, test.df)
# process features to be used as column names for the tidy dataset
hdr <- read.table(filename, row.names = NULL)[idx, 2]
hdr <- gsub("-", ".", hdr)
hdr <- gsub("\\(\\)", "", hdr)
hdr <- sub("^[t]", "Time", hdr)
hdr <- sub("^[f]", "Freq", hdr)
hdr <- sub("(Body)+", "Body", hdr)
colnames(clean.df) <- c("subject.id", "activity", hdr)
# ordering tidy dataset rows by subject then activity
clean.df <- clean.df[order(clean.df$subject.id, clean.df$activity), ]
row.names(clean.df) <- NULL
# garbage collection:
# remove unnecessary items from environment to free memory
rm(temp.file, conn, hdr, idx, train.df, test.df)
# saves the processed dataset for future analysis
write.csv(clean.df, file = "processed_data.csv", row.names = FALSE)
# uses the aggregate function to summarize clean.df with means of
# data collected for each subject and activity
tidy.df <- with(clean.df,
aggregate(clean.df[c(seq(3, dim(clean.df)[2]))],
by = list(subject.id, activity),
mean))
colnames(tidy.df) [1]<- "subject"
colnames(tidy.df)[2] <- "activity"
# saves the aggregated dataset for reporting
write.table(tidy.df, file = "tidy dataset.txt", row.names = FALSE)
## set working directory
setwd("G:/github/GettingAndCleaningData")
# folder with data
folder <- "UCI HAR Dataset"
# columns be read
# features data mean() and std()
filename <- sprintf("%s/features.txt", folder)
conn <- file(filename, "r")
idx <- grep("mean\\(\\)|std\\(\\)", readLines(conn))
close(conn)
# function read:
# this function to read files
make.read <- function(directory, index = idx){
subject.df <- read.table(sprintf("%s/%s/subject_%s.txt",
folder,directory, directory))
y.df <- read.table(sprintf("%s/%s/y_%s.txt",
folder, directory, directory))
X.df <- read.table(sprintf("%s/%s/X_%s.txt",
folder, directory, directory))[, index]
dataset <- cbind(subject.df, y.df, X.df)
# merge activity labels to sensor data
activity.df <- read.table(sprintf("%s/activity_labels.txt", folder))
suppressWarnings(dataset <- merge(dataset, activity.df, by.x = 2,
by.y = 1))
dataset[1] <- NULL
dataset <- dataset[c(1, dim(dataset)[2], seq(2, dim(dataset)[2]-1))]
dataset
}
# combined the train and test data frames
train.df <- make.read("train")
test.df <- make.read("test")
clean.df <- rbind(train.df, test.df)
# process features to be used as column names for the tidy dataset
hdr <- read.table(filename, row.names = NULL)[idx, 2]
hdr <- gsub("-", ".", hdr)
hdr <- gsub("\\(\\)", "", hdr)
hdr <- sub("^[t]", "Time", hdr)
hdr <- sub("^[f]", "Freq", hdr)
hdr <- sub("(Body)+", "Body", hdr)
colnames(clean.df) <- c("subject.id", "activity", hdr)
# ordering tidy dataset rows by subject then activity
clean.df <- clean.df[order(clean.df$subject.id, clean.df$activity), ]
row.names(clean.df) <- NULL
# saves the processed dataset for future analysis
write.csv(clean.df, file = "processed_data.csv", row.names = FALSE)
# uses the aggregate function to summarize
tidy.df <- with(clean.df,
aggregate(clean.df[c(seq(3, dim(clean.df)[2]))],
by = list(subject.id, activity),
mean))
colnames(tidy.df) [1]<- "subject"
colnames(tidy.df)[2] <- "activity"
# saves the aggregated dataset for reporting
write.table(tidy.df, file = "tidy dataset.txt", row.names = FALSE)
## set working directory
setwd("G:/github/GettingAndCleaningData")
# folder with data
folder <- "UCI HAR Dataset"
# columns be read
# features data mean() and std()
filename <- sprintf("%s/features.txt", folder)
conn <- file(filename, "r")
idx <- grep("mean\\(\\)|std\\(\\)", readLines(conn))
close(conn)
# function read:
# this function to read files
make.read <- function(directory, index = idx){
subject.df <- read.table(sprintf("%s/%s/subject_%s.txt",
folder,directory, directory))
y.df <- read.table(sprintf("%s/%s/y_%s.txt",
folder, directory, directory))
X.df <- read.table(sprintf("%s/%s/X_%s.txt",
folder, directory, directory))[, index]
dataset <- cbind(subject.df, y.df, X.df)
# merge activity labels to sensor data
activity.df <- read.table(sprintf("%s/activity_labels.txt", folder))
suppressWarnings(dataset <- merge(dataset, activity.df, by.x = 2,
by.y = 1))
dataset[1] <- NULL
dataset <- dataset[c(1, dim(dataset)[2], seq(2, dim(dataset)[2]-1))]
dataset
}
# combined the train and test data frames
train.df <- make.read("train")
test.df <- make.read("test")
clean.df <- rbind(train.df, test.df)
# process features to be used as column names for the tidy dataset
hdr <- read.table(filename, row.names = NULL)[idx, 2]
hdr <- gsub("-", ".", hdr)
hdr <- gsub("\\(\\)", "", hdr)
hdr <- sub("^[t]", "Time", hdr)
hdr <- sub("^[f]", "Freq", hdr)
hdr <- sub("(Body)+", "Body", hdr)
colnames(clean.df) <- c("subject.id", "activity", hdr)
# ordering tidy dataset rows by subject then activity
clean.df <- clean.df[order(clean.df$subject.id, clean.df$activity), ]
row.names(clean.df) <- NULL
# uses the aggregate function to summarize
tidy.df <- with(clean.df,
aggregate(clean.df[c(seq(3, dim(clean.df)[2]))],
by = list(subject.id, activity),
mean))
colnames(tidy.df) [1]<- "subject"
colnames(tidy.df)[2] <- "activity"
# saves the aggregated dataset for reporting
write.table(tidy.df, file = "tidy dataset.txt", row.names = FALSE)
## set working directory
setwd("G:/github/GettingAndCleaningData")
# folder with data
folder <- "UCI HAR Dataset"
# columns be read
# features data mean() and std()
filename <- sprintf("%s/features.txt", folder)
conn <- file(filename, "r")
idx <- grep("mean\\(\\)|std\\(\\)", readLines(conn))
close(conn)
# function read:
# this function to read files
make.read <- function(directory, index = idx){
subject.df <- read.table(sprintf("%s/%s/subject_%s.txt",
folder,directory, directory))
y.df <- read.table(sprintf("%s/%s/y_%s.txt",
folder, directory, directory))
X.df <- read.table(sprintf("%s/%s/X_%s.txt",
folder, directory, directory))[, index]
dataset <- cbind(subject.df, y.df, X.df)
# merge activity labels to sensor data
activity.df <- read.table(sprintf("%s/activity_labels.txt", folder))
suppressWarnings(dataset <- merge(dataset, activity.df, by.x = 2,
by.y = 1))
dataset[1] <- NULL
dataset <- dataset[c(1, dim(dataset)[2], seq(2, dim(dataset)[2]-1))]
dataset
}
# combined the train and test data frames
train.df <- make.read("train")
test.df <- make.read("test")
clean.df <- rbind(train.df, test.df)
# process features to be used as column names for the tidy dataset
hdr <- read.table(filename, row.names = NULL)[idx, 2]
hdr <- gsub("-", ".", hdr)
hdr <- gsub("\\(\\)", "", hdr)
hdr <- sub("^[t]", "Time", hdr)
hdr <- sub("^[f]", "Freq", hdr)
hdr <- sub("(Body)+", "Body", hdr)
colnames(clean.df) <- c("subject.id", "activity", hdr)
# ordering tidy dataset rows by subject then activity
clean.df <- clean.df[order(clean.df$subject.id, clean.df$activity), ]
row.names(clean.df) <- NULL
# uses the aggregate function to summarize
tidy.df <- with(clean.df,
aggregate(clean.df[c(seq(3, dim(clean.df)[2]))],
by = list(subject.id, activity),
mean))
colnames(tidy.df) [1]<- "subject"
colnames(tidy.df)[2] <- "activity"
# saves the aggregated dataset for reporting
write.table(tidy.df, file = "TidyData.txt", row.names = FALSE)
library(swirl)
install_from_swirl("Getting and Cleaning Data")
swirl()
path2csv
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
rm("mydf")
cran <- tbl_df(mydf)
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
cran <- tbl_df(mydf)
rm("mydf")
cran
select(cran, r_arch:country)
select(cran, country:r_arch)
select(cran, r_arch:country)
info()
?select
5:20
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size > 100500, r_os == "linux-gnu")
"!is.na(c(3, 5, NA, 10))"
is.na(c(3, 5, NA, 10))
"!is.na(c(3, 5, NA, 10))"
"!is.na(c(3, 5, NA, 10))"
is.na(c(3, 5, NA, 10))
is.na(c(3, 5, NA, 10))
omnitest('is.na(c(3, 5, NA, 10))')
is.na(c(3, 5, NA, 10))
"!is.na(c(3, 5, NA, 10))"
!is.na(c(3, 5, NA, 10))
filter(cran, !is.na(r_version))
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
cran
?group_by
by_package <- group_by(cran, package)
by_package
summarize(by_package, mean(size))
pack_sum
script_results_identical('pack_sum'); multi_expr_creates_var('pack_sum')
install.packages("pack_sum")
pack_sum
script_results_identical('pack_sum'); multi_expr_creates_var('pack_sum')
script_results_identical('pack_sum'); multi_expr_creates_var('pack_sum')
script_results_identical("pack_sum"); multi_expr_creates_var("pack_sum")
omnitest('summarize(by_package, mean(size))')
script_results_identical('pack_sum'); multi_expr_creates_var('pack_sum')
package("pack_sum")
script_results_identical('pack_sum');
# Compute four values, in the following order, from
# the grouped data:
#
# 1. count = n()
# 2. unique = n_distinct(ip_id)
# 3. countries = n_distinct(country)
# 4. avg_bytes = mean(size)
#
# A few thing to be careful of:
#
# 1. Separate arguments by commas
# 2. Make sure you have a closing parenthesis
# 3. Check your spelling!
# 4. Store the result in pack_sum (for 'package summary')
#
# You should also take a look at ?n and ?n_distinct, so
# that you really understand what is going on.
pack_sum <- summarize(by_package,
count = ,
unique = ,
countries = ,
avg_bytes = )
script_results_identical('pack_sum'); multi_expr_creates_var('pack_sum')
script_results_identical("pack_sum"); multi_expr_creates_var("pack_sum")
script_results_identical('pack_sum'); multi_expr_creates_var('pack_sum')
pack_sum
# Compute four values, in the following order, from
# the grouped data:
#
# 1. count = n()
# 2. unique = n_distinct(ip_id)
# 3. countries = n_distinct(country)
# 4. avg_bytes = mean(size)
#
# A few thing to be careful of:
#
# 1. Separate arguments by commas
# 2. Make sure you have a closing parenthesis
# 3. Check your spelling!
# 4. Store the result in pack_sum (for 'package summary')
#
# You should also take a look at ?n and ?n_distinct, so
# that you really understand what is going on.
pack_sum <- summarize(by_package,
count = ,
unique = ,
countries = ,
avg_bytes = )
pack_sum <- summarize(by_package,
count = n(),
unique = n_distinct(ip_id),
countries = n_distinct(country),
avg_bytes = mean(size))
script_results_identical('pack_sum'); multi_expr_creates_var('pack_sum')
script_results_identical("pack_sum"); multi_expr_creates_var("pack_sum")
script_results_identical('pack_sum'); multi_expr_creates_var('pack_sum')
pack_sum
submit()
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(pack_sum, count > 679)
top_counts
head(top_counts, 20)
arrange(top_counts, desc(count))
top_unique <- filter(pack_sum, unique > 465)
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique > 465)
top_unique
arrange(top_unique, desc(unique))
script_results_identical('result1'); multi_expr_creates_var('result1')
# Don't change any of the code below. Just type submit()
# when you think you understand it.
# We've already done this part, but we're repeating it
# here for clarity.
by_package <- group_by(cran, package)
pack_sum <- summarize(by_package,
count = n(),
unique = n_distinct(ip_id),
countries = n_distinct(country),
avg_bytes = mean(size))
# Here's the new bit, but using the same approach we've
# been using this whole time.
top_countries <- filter(pack_sum, countries > 60)
result1 <- arrange(top_countries, desc(countries), avg_bytes)
# Print the results to the console.
print(result1)
submit()
script_results_identical('result2'); multi_expr_creates_var('result2')
submit()
script_results_identical('result3'); multi_expr_creates_var('result3')
submit()
script_vals_identical()
script_vals_identical()
submit()
submit()
submit()
submit()
submit()
library(tidyr)
students
?gather
gather(students, sex, count, -grade)
students2
res <- gather(students2, sex_class, count, -grade)
res
?separate
separate(res, sex_class, c("sex", "class"))
script_vals_identical()
submit()
submit()
submit()
submit()
students3
script_vals_identical()
submit()
?spread
?spread
submit()
extract_numeric("class5")
extract_numeric("class5")
submit()
students4
submit()
submit()
script_vals_identical()
submit()
passed
failed
passed <- passed %>% mutate(status = "passed")
failed <- failed %>% mutate(status = "failed")
rbind_list(passed, failed)
sat
script_vals_identical()
submit()
submit()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package = lubridate)
this_day <- today()
this_day
month(this_day)
wday(this_day)
wday(this_day, label = TRUE)
this_moment <- now()
this_moment
second(this_moment)
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
mdy("March 12, 1975")
dmy(25081985)
ymd("192012")
ymd("1920/1/2")\
ymd("1920/1/2")
dt1
ymd_hms(dt1)
dt2
ymd(dt2)
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
this_moment <- update(this_moment, hours = 10, minutes = 16, seconds = 0)
this_moment
hms("03:22:14")
dt2
ymd(dt2)
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
this_moment <- update(this_moment, hours = 10, minutes = 16, seconds = 0)
this_moment
nyc <- now("America/New_York")
nyc
depart <- nyc + days(2)
depart
depart <- update(depart, hours = 17, minutes = 34)
depart
?with_tz
arrive <- with_tz(arrive, "Asia/Hong_Kong")
rrive <- depart + hours(15) + minutes(50)
arrive <- depart + hours(15) + minutes(50)
arrive <- depart + hours(15) + minutes(50)
expr_creates_var('arrive'); test_arrive_val()
arrive <- depart + hours(15) + minutes(50)
arrive <- depart + hours(15) + minutes(50)
arrive <- depart + hours(15) + minutes(50)
arrive <- depart + hours(15) + minutes(50)
depart
arrive <- (nyc + days(2)) + hours(15) + minutes(50)
info
info()
arrive <- depart + hours(15) + minutes(50)
depart <- update(depart, hours = 17, minutes = 34)
arrive <- depart + hours(15) + minutes(50)
arrive <- (depart + hours(15) + minutes(50))
arrive
expr_creates_var('arrive');
test_arrive_val()
arrive <- depart + hours(15) + minutes(50)
arrive <- depart
arrive
View(by_package)
arrive <- depart + hours(15) + minutes(50)
arrive <- depart + (hours(15) + minutes(50))
rm(arrive)
arrive <- depart + (hours(15) + minutes(50))
arrive <- update(depart, hours = 15, minutes = 50)
arrive <- 1
arrive <- update(depart, hours = 15, minutes = 50)
arrive <- depart
depart
arrive
rm(arrive)
arrive <- depart
arrive <- update(aarive, hours = 15, minutes = 50)
arrive <- update(arrive, hours = 15, minutes = 50)
arrive
info()
skip()
skip()
arrive <- with_tz(arrive, "Asia/Hong_Kong")
info()
skip()
bye()
